{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-21T05:30:18.073356Z",
     "start_time": "2025-01-21T05:30:17.281543Z"
    }
   },
   "source": "import cv2\n",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T05:30:26.619318Z",
     "start_time": "2025-01-21T05:30:26.614637Z"
    }
   },
   "cell_type": "code",
   "source": "print(cv2.__version__)",
   "id": "764d0a26699c0593",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T08:16:59.063407Z",
     "start_time": "2025-01-21T08:16:49.081173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0) # Initialize the video capture(0-default webcam)\n",
    "\n",
    "# Check if camera is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Loop to capture frames continuously\n",
    "while True:\n",
    "    ret, frame = cap.read() # Read a frame from camera\n",
    "\n",
    "    # Breaking the loop if frame reading fails\n",
    "    if not ret:\n",
    "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "        break\n",
    "\n",
    "    # Display the frame in the window\n",
    "    cv2.imshow('Real-time video feed', frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "9bbd975542decf5e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T12:30:59.519556Z",
     "start_time": "2025-01-23T12:30:19.084454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def is_in_restricted_zone(cx, cy, zone):\n",
    "    \"\"\"\n",
    "    Check if a point (cx, cy) is within the restricted zone.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = zone\n",
    "    return x1 <= cx <= x2 and y1 <= cy <= y2\n",
    "\n",
    "# Initialize background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=False)\n",
    "\n",
    "# Define the restricted zone (x1, y1, x2, y2)\n",
    "restricted_zone = (100, 100, 00, 4004)\n",
    "\n",
    "# Cooldown timer for alerts\n",
    "last_alert_time = 0\n",
    "alert_cooldown = 2  # seconds\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame for better performance (optional)\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "\n",
    "    # Apply thresholding to the mask to clean up noise\n",
    "    _, thresh = cv2.threshold(fg_mask, 50, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Remove noise using morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    # Find contours of the motion\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Draw the restricted zone on the frame\n",
    "    cv2.rectangle(frame, (restricted_zone[0], restricted_zone[1]),\n",
    "                  (restricted_zone[2], restricted_zone[3]), (0, 0, 255), 2)\n",
    "\n",
    "    motion_detected = False  # Flag to track motion in the restricted zone\n",
    "\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) < 700:  # Ignore small movements\n",
    "            continue\n",
    "\n",
    "        # Calculate the bounding box and center of motion\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        cx, cy = x + w // 2, y + h // 2\n",
    "\n",
    "        # Check if the motion is in the restricted zone\n",
    "        if is_in_restricted_zone(cx, cy, restricted_zone):\n",
    "            motion_detected = True\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, \"MOTION DETECTED!\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # Trigger alert with cooldown\n",
    "            current_time = time.time()\n",
    "            if current_time - last_alert_time > alert_cooldown:\n",
    "                print(\"ALERT: Motion detected in restricted zone!\")\n",
    "                last_alert_time = current_time\n",
    "\n",
    "    # If no motion is detected in the restricted zone, ensure the alert is off\n",
    "    if not motion_detected:\n",
    "        cv2.putText(frame, \"No Motion in Restricted Zone\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "    # Display the frames\n",
    "    cv2.imshow(\"Live Feed\", frame)\n",
    "    cv2.imshow(\"Threshold Frame\", thresh)\n",
    "\n",
    "    # Break on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "5de87d1f4177e15f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALERT: Motion detected in restricted zone!\n",
      "ALERT: Motion detected in restricted zone!\n",
      "ALERT: Motion detected in restricted zone!\n",
      "ALERT: Motion detected in restricted zone!\n",
      "ALERT: Motion detected in restricted zone!\n",
      "ALERT: Motion detected in restricted zone!\n",
      "ALERT: Motion detected in restricted zone!\n",
      "ALERT: Motion detected in restricted zone!\n",
      "ALERT: Motion detected in restricted zone!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T11:45:30.740122Z",
     "start_time": "2025-01-28T11:45:27.973870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import the necessary libraries\n",
    "import cv2\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Define a dictionary mapping human body parts to their corresponding indices in the model's output\n",
    "BODY_PARTS = {\n",
    "    \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "    \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "    \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "    \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18\n",
    "}\n",
    "\n",
    "# Define a list of pairs representing the body parts that should be connected to visualize the pose\n",
    "POSE_PAIRS = [\n",
    "    [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "    [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "    [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "    [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "    [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"]\n",
    "]\n",
    "\n",
    "# Specify the input dimensions for the neural network\n",
    "width = 368\n",
    "height = 368\n",
    "inWidth = width\n",
    "inHeight = height\n",
    "\n",
    "# Load the pre-trained OpenPose model from a file\n",
    "net = cv.dnn.readNetFromTensorflow(r\"E:\\Optimizing Workplace Compliance and Safety\\human-pose-estimation-opencv\\graph_opt.pb\")\n",
    "thr = 0.2  # Set a confidence threshold for detecting keypoints\n",
    "\n",
    "# Define a function to detect poses in an input frame\n",
    "def poseDetector(frame):\n",
    "    frameWidth = frame.shape[1]\n",
    "    frameHeight = frame.shape[0]\n",
    "\n",
    "    # Prepare the input for the model by resizing and mean normalization\n",
    "    net.setInput(cv.dnn.blobFromImage(frame, 1.0, (inWidth, inHeight), (127.5, 127.5, 127.5), swapRB=True, crop=False))\n",
    "    out = net.forward()\n",
    "    out = out[:, :19, :, :]  # Extract the first 19 elements, corresponding to the body part keypoints\n",
    "\n",
    "    # Ensure the number of detected body parts matches the predefined BODY_PARTS\n",
    "    assert(len(BODY_PARTS) == out.shape[1])\n",
    "\n",
    "    points = []  # Initialize a list to hold the detected keypoints\n",
    "    # Iterate over each body part to find the keypoints\n",
    "    for i in range(len(BODY_PARTS)):\n",
    "        # Extract the heatmap for the current body part\n",
    "        heatMap = out[0, i, :, :]\n",
    "        # Find the point with the maximum confidence\n",
    "        _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "        # Scale the point's coordinates back to the original frame size\n",
    "        x = (frameWidth * point[0]) / out.shape[3]\n",
    "        y = (frameHeight * point[1]) / out.shape[2]\n",
    "        # Add the point to the list if its confidence is above the threshold\n",
    "        points.append((int(x), int(y)) if conf > thr else None)\n",
    "\n",
    "    # Draw lines and ellipses to represent the pose in the frame\n",
    "    for pair in POSE_PAIRS:\n",
    "        partFrom = pair[0]\n",
    "        partTo = pair[1]\n",
    "        # Ensure the body parts are in the BODY_PARTS dictionary\n",
    "        assert(partFrom in BODY_PARTS)\n",
    "        assert(partTo in BODY_PARTS)\n",
    "\n",
    "        idFrom = BODY_PARTS[partFrom]\n",
    "        idTo = BODY_PARTS[partTo]\n",
    "\n",
    "        # If both keypoints are detected, draw the line and keypoints\n",
    "        if points[idFrom] and points[idTo]:\n",
    "            cv.line(frame, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "            cv.ellipse(frame, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "            cv.ellipse(frame, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "    t, _ = net.getPerfProfile()  # Optional: Retrieve the network's performance profile\n",
    "\n",
    "    return frame  # Return the frame with the pose drawn\n",
    "\n",
    "# Load an input image\n",
    "input = cv.imread(r\"E:\\Optimizing Workplace Compliance and Safety\\Data\\pic2.jpg\")\n",
    "# Pass the image to the poseDetector function\n",
    "output = poseDetector(input)\n",
    "# Display the output image with the detected pose\n",
    "cv2.imshow(\"Pose Estimation\", output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "158eea3de35a951d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e7e9de9bacec55d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
